{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots for the Teachable Robot Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dirs = [\n",
    "    pathlib.Path('data')\n",
    "]\n",
    "\n",
    "def load_data(name):\n",
    "    name = str(name)\n",
    "    name_paths = []\n",
    "    for data_dir in data_dirs:\n",
    "        for exp_dir in data_dir.iterdir():\n",
    "            if name in exp_dir.name:\n",
    "                name_paths.append(exp_dir)\n",
    "    assert len(name_paths) > 0, \"No files found with name \" + name\n",
    "    assert len(name_paths) == 1, f\"Found multiple files with name {name}: {[path.name for path in name_paths]}\"\n",
    "    csv_name = name_paths[0].joinpath('progress.csv')\n",
    "    data = pd.read_csv(csv_name)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTES\n",
    "<font color='red'>\n",
    "Runs to start:\n",
    "    1. Once we finish finalizing hparams, run all the other teachers (no distillation).\n",
    "    2. Once we finish finalizing hparams, run all the other teachers (yes distillation).\n",
    "    \n",
    "Things to think through:\n",
    "    \n",
    "Code things to change:\n",
    "    1. Num feedback counter\n",
    "    2. Write a function to try a bunch of other levels and see which levels the policy can perform well on.\n",
    "    \n",
    "Plots to make/add\n",
    "1. Load from tb\n",
    "2. C0 - sample efficiency\n",
    "3. C0 - num feedback\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Claim 0: all forms of feedback help the agent learn more quickly (no distillation) than no teacher\n",
    "\n",
    "This should be true, so long as the feedback contains any useful information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(10)\n",
    "pa_fake = np.linspace(0, 1, 10) + np.random.uniform(0, 1, 10) / 5\n",
    "cc2_fake = np.linspace(0, 1, 10) + np.random.uniform(0, 1, 10) / 5\n",
    "cc4_fake = np.linspace(0, 1, 10) + np.random.uniform(0, 1, 10) / 5\n",
    "sub_fake = np.linspace(0, 1, 10) + np.random.uniform(0, 1, 10) / 5\n",
    "reward_fake = np.linspace(0, 1, 10) + np.random.uniform(0, 1, 10) / 5\n",
    "demos_fake = np.linspace(0, 1, 10) + np.random.uniform(0, 1, 10) / 5\n",
    "\n",
    "plt.plot(x, pa_fake)\n",
    "plt.plot(x, cc2_fake)\n",
    "plt.plot(x, cc4_fake)\n",
    "plt.plot(x, sub_fake)\n",
    "plt.plot(x, reward_fake)\n",
    "plt.plot(x, demos_fake)\n",
    "\n",
    "plt.title(\"Training RL With Teacher - Sample Efficiency\")\n",
    "plt.ylabel('Curriculum %')\n",
    "plt.xlabel('Samples')\n",
    "plt.legend(['PA', 'CC2', 'CC4', 'Sub', 'Reward Only', 'Demos'])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(x, pa_fake)\n",
    "plt.plot(x, cc2_fake)\n",
    "plt.plot(x, cc4_fake)\n",
    "plt.plot(x, sub_fake)\n",
    "plt.plot(x, reward_fake)\n",
    "plt.plot(x, demos_fake)\n",
    "\n",
    "plt.title(\"Training RL With Teacher - Feedback Efficiency\")\n",
    "plt.ylabel('Curriculum %')\n",
    "plt.xlabel('Feedback Units')\n",
    "plt.legend(['PA', 'CC2', 'CC4', 'Sub', 'Reward Only', 'Demos'])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# TABLE VERSION OF THIS\n",
    "x = ['PA', 'CC2', 'CC4', 'Sub', 'Reward Only', 'Demos']\n",
    "y = [10, 20, 30, 40, 50, 60]\n",
    "plt.ylabel(\"Feeback to complete curriculum\")\n",
    "plt.bar(x, y)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Claim 1: all forms of feedback help the agent learn a policy which does NOT require feedback than no teacher\n",
    "\n",
    "This should be true, so long as the agent actually completes the curriculum, since the version w/o teacher does not.\n",
    "\n",
    "If this isn't true for certain higher-level teachers that's fine, so long as we are able to show is that the problem is grounding, but once grounding it does work better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Claim 2: Policies conditioned on feedback can generalize zero-shot (or few-shot) to performing new tasks with the teacher.\n",
    "\n",
    "(Assume no new feedback vocab.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Claim 3: With the distillation process, we can generalize to performing new tasks without using the reward (much). This process is more feedback-efficient than just providing rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Claim 4: Lower-level communication is more sample-efficient + feedback-efficient to ground than higher-level.\n",
    "\n",
    "The goal is to motivate why we want to bootstrap rather than using the higher-level teachers directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Claim 5: Higher level communication is most feedback-efficient on new levels once grounded.\n",
    "\n",
    "The goal is to motivate why we want higher-level teachers at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Claim 6: We can ground higher-level feedback using lower-level feedback and minimal use of the reward. This is more efficient than grounding the higher-level communication directly.\n",
    "\n",
    "Goal is to show that bootstrapping is a reasonable thing to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leaderboard - Curriculum Learning\n",
    "\n",
    "Keep track of the current best methods of learning a curriculum (ranked by feedback-efficiency)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leaderboard - Few-Shot Generalization\n",
    "\n",
    "\n",
    "Keep track of the current best methods of few-shot learning heldout levels (ranked by feedback-efficiency)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
